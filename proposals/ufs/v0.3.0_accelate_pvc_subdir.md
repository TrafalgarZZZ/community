使用Fluid加速已有的PVC。部分数据无法直接通过Alluxio mount，通过pvc的方式间接使用Fluid加速。

## Motivation

## Problems

1. Fluid需要保持和PVC一样的亲和性和replicas吗？

> 集群有四个节点（a, b, c, d），PVC1分布在节点a和b上。那Fluid嵌套在PVC1上的PVC2是否也应该只部署在节点（a，b）上？还是说创建者有其他需求，比如说通过Fluid扩展PVC1到其他节点（c，d）？

2. 加速PVC和hostpath在设计上的区别

> Fluid加速hostpath的设计，是在alluxio-master节点上挂载hostpath到ufs。这种做法在hostpath上应用良好，因为hostpath一般只有一份。
>
> 但对PVC来说，如1中例子，PVC1在节点a和b上都有部署，如果只在a上部署一个alluxio-master，那b中的pvc就完全使用不到了。这样的话，Alluxio是不是只会从master节点上拉取数据，而master节点又会从pvc拉取数据。
>
> worker上的ufs。

3. 更细粒度的PVC数据管理

把PVC看作一份独立的文件系统，其实我们可以更精细地管理想要Fluid加速的部分文件。

4. 权限问题

pvc有所属的用户和组，非root，root

- mount命令会发生权限问题
- container需要flag `--privileged=true`
- 软连接会发生用户和组变化（仅是显示上的变化）

5. Alluxio何时挂载

- 启动前设置

```bash
bash-4.4# mount --bind /underFSStorage/oss-data/train /opt/alluxio/underFSStorage/train
mount: permission denied (are you root?)
bash-4.4# sudo mount --bind /underFSStorage/oss-data/train /opt/alluxio/underFSStorage/train
bash: sudo: command not found
bash-4.4# ln -s /underFSStorage/oss-data/train /opt/alluxio/underFSStorage/train
bash-4.4# ls -lht /opt/alluxio/underFSStorage/train
total 0
lrwxrwxrwx    1 root     root          30 Sep 13 08:37 train -> /underFSStorage/oss-data/train
bash-4.4# ls -lht /opt/alluxio/underFSStorage/train/
total 0
lrwxrwxrwx    1 root     root          30 Sep 13 08:37 train -> /underFSStorage/oss-data/train
bash-4.4# ls -lht /underFSStorage/oss-data/
total 1
drwxr-x---    1 1005     1005           0 Jan  7  2020 train
```

## Goals

- 支持pvc子目录挂载

## API

### Custom Resource Definition

#### Dataset

```yaml
apiVersion: data.fluid.io/v1alpha1
kind: Dataset
metadata:
  name: oss
spec:
  mounts:
    - mountPoint: pvc://lustre-pv/path/to/subdir/
      name: lustre
    - mountPoint: pvc://nfs-pv/path/to/subdir/
      name: nfs
```

## Design

### 子目录挂载

1. 用户在Dataset中指定`pvc://lustre-pv/path/to/subdir/`格式的pvc及其子路径
2. Fluid在transform时，发现`MountPoint`若是pvc，并且指定了子路径，则：
   1. transform时，将所有pvc挂载到`/pvcs`目录
   2. 启动master和worker前(entrypoint执行前？)，需要在他们的pod里挂载pvc子路径到`/underStorage`；或者等master和worker启动后，再主动mount到Alluxio？

## Test

### 准备数据

在nfs server上

```bash
mkdir -p /mnt/nfs-src
mkdir -p /mnt/nfs-src/hbase /mnt/nfs-src/hive
wget -P /nfs-src/hbase https://mirrors.nju.edu.cn/apache/hbase/2.2.5/hbase-2.2.5-bin.tar.gz
wget -P /nfs-src/hive  https://mirrors.nju.edu.cn/apache/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz
groupadd -g 1005 fluid-user-1 && \
useradd -u 1005  -g fluid-user-1  fluid-user-1 && \
usermod -a -G root fluid-user-1
groupadd -g 1006 fluid-user-2 && \
useradd -u 1006  -g fluid-user-2  fluid-user-2 && \
usermod -a -G root fluid-user-2
chown -R fluid-user-1:fluid-user-1 /nfs-src/hbase
chown -R fluid-user-2:fluid-user-2 /nfs-src/hive
```

目录组织：

```bash
$ ls -lhtR /nfs-src/
/nfs-src/:
total 8.0K
drwxr-xr-x 2 fluid-user-2 fluid-user-2 4.0K Sep 14 15:34 hive
drwxr-xr-x 2 fluid-user-1 fluid-user-1 4.0K Sep 14 15:33 hbase

/nfs-src/hive:
total 266M
-rw-r--r-- 1 fluid-user-2 fluid-user-2 266M Aug 27  2019 apache-hive-3.1.2-bin.tar.gz

/nfs-src/hbase:
total 211M
-rw-r--r-- 1 fluid-user-1 fluid-user-1 211M May 26 15:30 hbase-2.2.5-bin.tar.gz
```

### 配置nfs

[nfs配置教程]([CentOS 7 下 yum 安装和配置 NFS - Zhanming's blog](https://qizhanming.com/blog/2018/08/08/how-to-install-nfs-on-centos-7))

```bash
$ sudo vi /etc/exports
/nfs-src     192.168.1.0/24(rw,sync,no_root_squash,no_all_squash)
$ exportfs -rv
exporting 192.168.1.0/24:/nfs-src
```

### 创建nfs pvc

```bash
$ cat << EOF >> nfs.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: slow
  mountOptions:
  - hard
  - nfsvers=4.1
  nfs:
    path: /nfs-src
    server: 192.168.1.11
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pv
  annotations:
    volume.beta.kubernetes.io/storage-class: "slow"
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
EOF
```

```bash
kubectl create -f nfs.yaml
```

### 创建Dataset

```bash
cat << EOF >> dataset.yaml
apiVersion: data.fluid.io/v1alpha1
kind: Dataset
metadata:
  name: nested-nfs
spec:
  mounts:
    - mountPoint: pvc://nfs-pv
      name: nfs-pv
#  owner:
#    uid: 1111
#    gid: 1111
#  nodeAffinity:
#    required:
#      nodeSelectorTerms:
#        - matchExpressions:
#            - key: nonroot
#              operator: In
#              values:
#                - "true"
---
apiVersion: data.fluid.io/v1alpha1
kind: AlluxioRuntime
metadata:
  name: nested-nfs
spec:
  replicas: 4
  tieredstore:
    levels:
      - mediumtype: MEM
        path: /dev/shm
        quota: 2Gi
        high: "0.95"
        low: "0.7"
#  runAs:
#    uid: 1111
#    gid: 1111
  properties:
    alluxio.user.file.writetype.default: MUST_CACHE
    alluxio.master.journal.folder: /journal
    alluxio.master.journal.type: UFS
    alluxio.user.block.size.bytes.default: 256MB
    alluxio.user.streaming.reader.chunk.size.bytes: 256MB
    alluxio.user.local.reader.chunk.size.bytes: 256MB
    alluxio.worker.network.reader.buffer.size: 256MB
    alluxio.user.streaming.data.timeout: 300sec
  master:
    jvmOptions:
      - "-Xmx4G"
  worker:
    jvmOptions:
      - "-Xmx4G"
  fuse:
    jvmOptions:
      - "-Xmx4G "
      - "-Xms4G "
    args:
      - fuse
      - --fuse-opts=kernel_cache,ro,max_read=131072,attr_timeout=7200,entry_timeout=7200,nonempty
EOF
```

```bash
kubectl create -f dataset.yaml
```

### 创建App

```bash
cat << EOF >> app.yaml
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 1
  serviceName: "nginx"
  podManagementPolicy: "Parallel"
  selector: # define how the deployment finds the pods it manages
    matchLabels:
      app: nginx
  template: # define the pods specifications
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx
          volumeMounts:
            - mountPath: /data
              name: nested-nfs
      volumes:
        - name: nested-nfs
          persistentVolumeClaim:
            claimName: nested-nfs
EOF
```

```bash
kubectl create -f app.yaml
```

```bash
$ kubectl exec -it nginx-0 bash
$ ls -lhtR /data
/data:
total 512
drwxr-xr-x 1 root root 2 Sep 14 07:34 nfs-pv

/data/nfs-pv:
total 1.0K
drwxr-xr-x 1 1006 1006 1 Sep 14 07:34 hive
drwxr-xr-x 1 1005 1005 1 Sep 14 07:33 hbase

/data/nfs-pv/hive:
total 266M
-rw-r--r-- 1 1006 1006 266M Aug 26  2019 apache-hive-3.1.2-bin.tar.gz

/data/nfs-pv/hbase:
total 211M
-rw-r--r-- 1 1005 1005 211M May 26 07:30 hbase-2.2.5-bin.tar.gz
```

> 问题1： 如果host上没有UID为1005和1006的用户，则用户和组会变为` nobody nogroup`

