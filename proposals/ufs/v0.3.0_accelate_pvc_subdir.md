<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [Motivation](#motivation)
- [Goals](#goals)
- [Non-Goals](#non-goals)
- [Goals](#goals-1)
- [API](#api)
  - [Custom Resource Definition](#custom-resource-definition)
    - [Dataset](#dataset)
- [Workflow](#workflow)
- [Q&A](#qa)
- [Demo](#demo)
  - [准备数据](#%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE)
  - [配置nfs](#%E9%85%8D%E7%BD%AEnfs)
  - [创建nfs pvc](#%E5%88%9B%E5%BB%BAnfs-pvc)
  - [创建Dataset](#%E5%88%9B%E5%BB%BAdataset)
  - [创建App](#%E5%88%9B%E5%BB%BAapp)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

## Motivation

Fluid V3.0 已经支持加速已有的PVC，但PVC可能包含属于不同用户的众多文件，我们有时间只希望加速其中的部分文件。

## Goals

- 更加细粒度的PVC数据管理，精确地指定加速PVC中的部分目录。
- 只读

## Non-Goals

- 加速一个PVC下的多个路径

## API

### Custom Resource Definition

#### Dataset

```yaml
apiVersion: data.fluid.io/v1alpha1
kind: Datasets
metadata:
  name: nested-nfs
spec:
  mounts:
    - mountPoint: pvc://nfs-pv/path/to/source
      name: nfs-pv
```

## Workflow

1. Fluid Runtime Controller识别到pvc://协议，并且pvc指定了子目录，就会将这个pvc挂载到缓存引擎的`/pvcs`路径下（若是未指定子目录的PVC，则是直接挂到`/underFSStorage`路径下，这是缓存引擎的Local Storage）
2. 缓存引擎的Master节点启动后，在Master节点**软链接**PVC子路径到`/underFSStorage`目录下
3. 随后Master节点加载元数据，检测到Local Storage下的PVC子路径
4. Fluid启动缓存引擎的Worker节点后，在所有Worker节点上也创建同样的软链接

## Q&A

1. 为什么选择软链接？

   软链接和`mount --bind`都能达到同样的效果，但最终选择了软链接的方式，这是考虑到：

   - `mount --bind`需要container指定flag `--privileged=true`（Alluxio Master和Worker默认赋予container这个flag，保持尽量少的修改原则），而软链接不需要
   - `mount --bind`可能发生的权限问题？
   
2. 修改缓存引擎接口？

   无论在加速hostpath还是远程数据集时都不需要特意在缓存引擎的worker上进行额外操作，这是因为：

   - 元数据加载只需要在master上进行（从而与worker解耦）；

   - hostpath或者非子路径的PVC只需要直接挂载到Local Storage上就行了，这是在container启动的时候就完成的。

   但是在进行PVC子路径加速时，我们只能先把整个PVC挂载到容器里，在精确地挂载PVC下的指定子路径到Local Storage，这就不得不在worker启动后进行`bind mount`或者创建软链接。

## Demo

### 准备数据

在nfs server上

```bash
mkdir -p /mnt/nfs-src
mkdir -p /mnt/nfs-src/hbase /mnt/nfs-src/hive
wget -P /nfs-src/hbase https://mirrors.nju.edu.cn/apache/hbase/2.2.5/hbase-2.2.5-bin.tar.gz
wget -P /nfs-src/hive  https://mirrors.nju.edu.cn/apache/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz
groupadd -g 1005 fluid-user-1 && \
useradd -u 1005  -g fluid-user-1  fluid-user-1 && \
usermod -a -G root fluid-user-1
groupadd -g 1006 fluid-user-2 && \
useradd -u 1006  -g fluid-user-2  fluid-user-2 && \
usermod -a -G root fluid-user-2
chown -R fluid-user-1:fluid-user-1 /nfs-src/hbase
chown -R fluid-user-2:fluid-user-2 /nfs-src/hive
```

目录组织：

```bash
$ ls -lhtR /nfs-src/
/nfs-src/:
total 8.0K
drwxr-xr-x 2 fluid-user-2 fluid-user-2 4.0K Sep 14 15:34 hive
drwxr-xr-x 2 fluid-user-1 fluid-user-1 4.0K Sep 14 15:33 hbase

/nfs-src/hive:
total 266M
-rw-r--r-- 1 fluid-user-2 fluid-user-2 266M Aug 27  2019 apache-hive-3.1.2-bin.tar.gz

/nfs-src/hbase:
total 211M
-rw-r--r-- 1 fluid-user-1 fluid-user-1 211M May 26 15:30 hbase-2.2.5-bin.tar.gz
```

### 配置nfs

[nfs配置教程]([CentOS 7 下 yum 安装和配置 NFS - Zhanming's blog](https://qizhanming.com/blog/2018/08/08/how-to-install-nfs-on-centos-7))

```bash
$ sudo vi /etc/exports
/nfs-src     192.168.1.0/24(rw,sync,no_root_squash,no_all_squash)
$ exportfs -rv
exporting 192.168.1.0/24:/nfs-src
```

### 创建nfs pvc

```bash
$ cat << EOF >> nfs.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
  - ReadOnlyMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: slow
  mountOptions:
  - hard
  - nfsvers=4.1
  nfs:
    path: /nfs-src
    server: 192.168.1.11
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pv
  annotations:
    volume.beta.kubernetes.io/storage-class: "slow"
spec:
  accessModes:
  - ReadOnlyMany
  resources:
    requests:
      storage: 1Gi
EOF
```

```bash
kubectl create -f nfs.yaml
```

### 创建Dataset

```bash
$ cat << EOF >> dataset.yaml
apiVersion: data.fluid.io/v1alpha1
kind: Dataset
metadata:
  name: nested-nfs
spec:
  mounts:
    - mountPoint: pvc://nfs-pv/hbase
      name: nfs-pv
---
apiVersion: data.fluid.io/v1alpha1
kind: AlluxioRuntime
metadata:
  name: nested-nfs
spec:
  replicas: 1
  tieredstore:
    levels:
      - mediumtype: MEM
        path: /dev/shm
        quota: 2Gi
        high: "0.95"
        low: "0.7"
  properties:
    alluxio.user.file.writetype.default: MUST_CACHE
    alluxio.master.journal.folder: /journal
    alluxio.master.journal.type: UFS
    alluxio.user.block.size.bytes.default: 256MB
    alluxio.user.streaming.reader.chunk.size.bytes: 256MB
    alluxio.user.local.reader.chunk.size.bytes: 256MB
    alluxio.worker.network.reader.buffer.size: 256MB
    alluxio.user.streaming.data.timeout: 300sec
  master:
    jvmOptions:
      - "-Xmx4G"
  worker:
    jvmOptions:
      - "-Xmx4G"
  fuse:
    jvmOptions:
      - "-Xmx4G "
      - "-Xms4G "
    args:
      - fuse
      - --fuse-opts=kernel_cache,ro,max_read=131072,attr_timeout=7200,entry_timeout=7200,nonempty
EOF
```

```bash
kubectl create -f dataset.yaml
```

### 创建App

```bash
$ cat << EOF >> app.yaml
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 1
  serviceName: "nginx"
  podManagementPolicy: "Parallel"
  selector: # define how the deployment finds the pods it manages
    matchLabels:
      app: nginx
  template: # define the pods specifications
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx
          volumeMounts:
            - mountPath: /data
              name: nested-nfs
      volumes:
        - name: nested-nfs
          persistentVolumeClaim:
            claimName: nested-nfs
EOF
```

```bash
kubectl create -f app.yaml
```

```bash
$ kubectl exec -it nginx-0 bash
$ ls -lhtR /data
root@nginx-0:/# ls -lhtR /data
/data:
total 512
drwxr-xr-x 1 1005 1005 1 Sep 19 08:36 hbase

/data/hbase:
total 211M
-rw-r--r-- 1 1005 1005 211M May 26 07:30 hbase-2.2.5-bin.tar.gz
```

> 注意： 如果host上没有UID为1005和1006的用户，则用户和组会变为` nobody nogroup`

